{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA en EigenFaces\n",
    "Laura M. Tejada López\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:green\">La teoría de PCA fue desarrollada por Pearson en 1901</span>\n",
    "- <span style=\"color:green\">Independientemente también por Hotelling en 1930's</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El Análisis de Componentes Principales (PCA) se utiliza usualmente como una técnica de visualización, reducción de dimensión y también como un paso intermedio en el análisis de datos como técnica explicatoria. Es por ello que este método es uno de los usos principales de la SVD (descomposición en valores singulares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La idea de PCA es que dado algún vector $x = (X_1, \\dots, X_p)$, buscamos un conjunto de combinaciones lineales $Y_i = a^{T}_ix$, $i = 1,2, \\dots, p$, de forma tal que la variabilidad de $k$ de estos sea aproximadamente la variabilidad correspondiente a todo el vector.\n",
    " \n",
    "  Geométricamente, esta transformación representa la selección de un nuevo conjunto de coordenadas, que obtenemos mediante rotación y traslación del sistema original a ejes en donde se maximiza la varianza en cada dirección. Este nuevo sistema de coordenadas está determinado por las componentes principales (PCs) que son no correlacionadas (ortogonales) entre sí. Luego, las primeras $k$ componentes principales expanden un subespacio que contiene a *la mejor visualización* en $k$ dimensiones; es decir, una proyección vista en la dirección de más información. En este sentido, las coordenadas ortogonales que conforman el nuevo subespacio, aunque son no correlacionadas entre sí, tienen la máxima correlación con las observaciones (vector renglón).\n",
    "  \n",
    "  La transformación de componentes principales está dada por: $$y = P^{T}(x - \\mu)$$\n",
    "  \n",
    "  \n",
    " donde $P = [e_1 | \\dots | e_p]$ es la matriz de eigenvectores de $\\varSigma$, tal que $\\varSigma = P \\varLambda P^{T}$, <span style=\"color:red\"> y que bajo dicha transformación, los contornos de la distribución de las variables transformadas se pueden expresar como: $$y^{T} \\varLambda^{-1} y = k^{2}$$</span>\n",
    " \n",
    " \n",
    "<span style=\"color:red\"> Y en caso de que así sea, habrá casi la misma información en las $k$ combinaciones lineales como en las $p$ variables.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El problema de optimización en PCA consiste en resolver el siguiente sistema de problemas de optimización:\n",
    " \n",
    " $$\\max_{a_1^{T} a_1 = 1} a_1^{T} \\varSigma a_1 = Var(Y_1)$$\n",
    " \n",
    " \n",
    " $$\\max_{a_i^{T} a_i = 1} a_i^{T} \\varSigma a_i = Var(Y_i), i = 2, \\dots, n$$ <span style=\"color:green\">(Falta agregar la restricción $a_i^T \\varSigma a_k = 0, k<i$)</span>\n",
    " \n",
    "donde:\n",
    "- $x$ es un vector de tamaño $p \\times 1$ con matriz de covarianzas $\\varSigma$ y eigenvalores $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\lambda_p \\geq 0$.\n",
    "- $a_1, \\dots, a_p \\in \\mathbb{R}^p$ los coeficientes de las $p$ combinaciones lineales $Y_i = a_ix$\n",
    "- La norma $||a_i||=1$, para no incrementar de forma arbitraria $Var(Y_i)$\n",
    "\n",
    "<span style=\"color:green\">(Revisar)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La solución a este problema de optimización es: $$a_i^* = e_i, Var(Y_i) = \\lambda_i$$\n",
    " \n",
    " Los vectores y valores propios de $\\varSigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estandarización de variables. Calculamos la media, $\\bar{x}$, de los renglones y se lo restamos a la matriz $X$: $$\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n}{X_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "\n",
    "<span style=\"color:green\">(Falta terminar)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [databook](http://databookuw.com/databook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lo que está en rojo es posible que lo quite.\n",
    "2. Lo que está en azul lo voy a redactar con otras palabras.\n",
    "3. Lo que está en verde hay que completarlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
