{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA en EigenFaces\n",
    "Laura M. Tejada López\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La teoría de PCA fue desarrollada por Pearson en 1901\n",
    "- Independientemente también por Hotelling en 1930's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El Análisis de Componentes Principales (PCA) se utiliza usualmente como una técnica de visualización, reducción de dimensión y también como un paso intermedio en el análisis de datos como técnica explicatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dado algún vector $x = (X_1, \\dots, X_p)$, buscamos un conjunto de combinaciones lineales $Y_i = a^{T}_ix$, $i = 1,2, \\dots, p$, de forma tal que la variabilidad de $k$ de estos sea aproximadamente la variabilidad correspondiente a todo el vector.\n",
    " \n",
    "  Geométricamente, esta transformación representa la selección d eun nuevo conjunto de coordenadas, que obtenemos mediante rotación y traslación del sistema original a ejes en donde se maximiza la varianza en cada dirección. Así, las primeras $k$ componentes principales expanden un subespacio que contiene a *la mejor visualización* en $k$ dimensiones; es decir, una proyección vista en la dirección de más información.\n",
    "  \n",
    "  La transformación de componentes principales está dada por: $$y = P^{T}(x - \\mu)$$\n",
    " donde $P = [e_1 | \\dots | e_p]$ es la matriz de eigenvectores de $\\varSigma$ tal que $\\varSigma = P \\varLambda P^{T}$, y que bajo dicha transformación, los contornos de la distribución de las variables transformadas se pueden expresar como $$y^{T} \\varLambda^{-1} y = k^{2}$$\n",
    " \n",
    " Y en caso de que así sea, habrá casi la misma información en las $k$ combinaciones lineales como en las $p$ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El problema de optimización en PCA consiste en resolver el siguiente sistema de problemas de optimización:\n",
    " \n",
    " $$\\max_{a_1^{T} a_1 = 1} a_1^{T} \\varSigma a_1 = Var(Y_1)$$\n",
    " \n",
    " \n",
    " $$\\max_{a_i^{T} a_i = 1} a_i^{T} \\varSigma a_i = Var(Y_i), i = 2, \\dots, n$$ (Falta una restricción)\n",
    " \n",
    "donde:\n",
    "- $x$ es un vector de tamaño $p \\times 1$ con matriz de covarianzas $\\varSigma$ y eigenvalores $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\lambda_3 \\geq 0$.\n",
    "\n",
    "(Terminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estandarización de variables. Calculamos la media, $\\bar{x}$, de los renglones y se lo restamos a la matriz $X$: $$\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n}{X_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "\n",
    "(Falta terminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [databook](http://databookuw.com/databook.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
