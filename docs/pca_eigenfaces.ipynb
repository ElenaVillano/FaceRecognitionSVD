{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA en EigenFaces\n",
    "Laura M. Tejada López\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:green\">La teoría de PCA fue desarrollada por Pearson en 1901</span>\n",
    "- <span style=\"color:green\">Independientemente también por Hotelling en 1930's</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El **Análisis de Componentes Principales** (PCA) se utiliza usualmente como una técnica de visualización, reducción de dimensión y también como un paso intermedio en el análisis de datos como técnica explicatoria. Es por ello que este método es uno de los usos principales de la SVD (descomposición en valores singulares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La idea de PCA es que dado algún vector $x = (X_1, \\dots, X_p)$, buscamos un conjunto de combinaciones lineales $Y_i = a^{T}_ix$, $i = 1,2, \\dots, p$, de forma tal que la variabilidad de $k$, con $k < p$, de estos sea aproximadamente la variabilidad correspondiente a todo el vector.\n",
    " \n",
    "  Geométricamente, esta transformación representa la selección de un nuevo conjunto de coordenadas, que obtenemos mediante rotación y traslación del sistema original a ejes, en donde se maximiza la varianza en cada dirección. Este nuevo sistema de coordenadas está determinado por las componentes principales (CPs), que son no correlacionadas (ortogonales) entre sí. Luego, las primeras $k$ componentes principales expanden un subespacio que contiene a *la mejor visualización* en $k$ dimensiones; es decir, una proyección vista en la dirección de más información. En este sentido, las coordenadas ortogonales que conforman el nuevo subespacio, aunque son no correlacionadas entre sí, tienen la máxima correlación con las observaciones (vector renglón).\n",
    "  \n",
    "<span style=\"color:green\">  La transformación de componentes principales está dada por: $$y = P^{T}(x - \\mu)$$</span>\n",
    "  \n",
    "  \n",
    "<span style=\"color:green\"> donde $P = [e_1 | \\dots | e_p]$ es la matriz de eigenvectores de $\\varSigma$, tal que $\\varSigma = P \\varLambda P^{T}$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El problema de optimización en PCA consiste en resolver el siguiente sistema:\n",
    " \n",
    " $$\\max_{a_1^{T} a_1 = 1} a_1^{T} \\varSigma a_1 = Var(Y_1)$$\n",
    " \n",
    " \n",
    " $$\\max_{a_i^{T} a_i = 1} a_i^{T} \\varSigma a_i = Var(Y_i), i = 2, \\dots, n$$ <span style=\"color:green\">(Falta agregar la restricción $a_i^T \\varSigma a_k = 0, k<i$)</span>\n",
    " \n",
    "donde:\n",
    "- $x$ es un vector de tamaño $p \\times 1$ con matriz de covarianzas $\\varSigma$ y eigenvalores $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\lambda_p \\geq 0$.\n",
    "- $a_1, \\dots, a_p \\in \\mathbb{R}^p$ los coeficientes de las $p$ combinaciones lineales $Y_i = a_ix$\n",
    "- La norma $||a_i||=1$, para no incrementar de forma arbitraria $Var(Y_i)$, ya que esta varianza se podría incrementar arbitrariamente al multiplicar $a_i$ por alguna constante.\n",
    "- $a_i^T \\varSigma a_k = 0, k<i$ es equivalente a decir que $Cov(a_i^T X, a_k^T X) = 0, k<i$ (ya que buscamos que las componentes principales sean ortogonales).\n",
    "\n",
    "<span style=\"color:green\">(Revisar)</span>\n",
    "\n",
    " De modo que la primera componente principal es la combinación lineal con varianza máxima; es decir, que maximiza $Var(Y_1) = a_1^{T} \\varSigma a_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La solución a este problema de optimización es: $$a_i^* = e_i, Var(Y_i) = \\lambda_i$$\n",
    " \n",
    " Los vectores y valores propios de $\\varSigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notemos que $tr(\\varSigma) = \\sum_{i=1}^{p} \\lambda_i = \\sum_{i=1}^{p} Var(Y_i)$, por lo que la proporción de la varianza total explicada  de la i-ésima componente principal es: $$\\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p}, i = 1, \\dots, p$$\n",
    " \n",
    " Luego, si la mayor parte de la varianza total se puede atribuir a las primeras componentes, entonces con éstas componentes podemos *sustituir* a las $p$ variables, sin perder mucha información, a cambio de disminuir la dimensión de nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estandarización de variables. Calculamos la media, $\\bar{x}$, de los renglones y se lo restamos a la matriz $X$: $$\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n}{X_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "\n",
    "<span style=\"color:green\">(Falta terminar)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elección de las Componentes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No existe una regla definitiva para determinar el número de componentes principales con las que resolveremos cada problema particular, pero en general, podemos utilizar las siguientes guías:\n",
    "\n",
    "- Quedarnos con las CP que acumulen cierto porcentaje de la variación total.\n",
    "- Quedarnos con las CP cuyos eigenvalores sean mayores que el promedio $\\bar{\\lambda}$.\n",
    "- Utilizar una gráfica de codo (*scree plot*).\n",
    "- Verificar la significancia de las componentes más *grandes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación de las Componentes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y Desventajas de PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas**:\n",
    "\n",
    "\n",
    "**Desventajas**:\n",
    "- Las componentes principales pueden ser sensibles a outliers.\n",
    "- Las componentes principales son sensibles a distintos escalamientos de los datos originales (obtendemos distintas CP según las transformaciones que hagamos a los datos), puesto que el porcentaje de varianza explicado será distinto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [databook](http://databookuw.com/databook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lo que está en rojo es posible que lo quite.\n",
    "2. Lo que está en azul lo voy a redactar con otras palabras.\n",
    "3. Lo que está en verde hay que completarlo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
