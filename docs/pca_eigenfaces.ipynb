{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA en EigenFaces\n",
    "Laura M. Tejada López\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El **Análisis de Componentes Principales** (PCA) se utiliza usualmente como una técnica de visualización, reducción de dimensión y también como un paso intermedio en el análisis de datos como técnica explicatoria. Es por ello que este método es uno de los usos principales de la SVD (descomposición en valores singulares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La idea de PCA es que dado algún vector $x = (X_1, \\dots, X_p)$, buscamos un conjunto de combinaciones lineales $Y_i = a^{T}_ix$, $i = 1,2, \\dots, p$, de forma tal que la variabilidad de $k$, con $k < p$, de estos sea aproximadamente la variabilidad correspondiente a todo el vector.\n",
    " \n",
    "  Geométricamente, esta transformación representa la selección de un nuevo conjunto de coordenadas, que obtenemos mediante rotación y traslación del sistema original a ejes, en donde se maximiza la varianza en cada dirección. Este nuevo sistema de coordenadas está determinado por las componentes principales (CPs), que son no correlacionadas (ortogonales) entre sí. Luego, las primeras $k$ componentes principales expanden un subespacio que contiene a *la mejor visualización* en $k$ dimensiones; es decir, una proyección vista en la dirección de más información. En este sentido, las coordenadas ortogonales que conforman el nuevo subespacio, aunque son no correlacionadas entre sí, tienen la máxima correlación con las observaciones (vector renglón).\n",
    "  \n",
    " La transformación de componentes principales está dada por: $$y = P^{T}(x - \\mu)$$\n",
    "  \n",
    "  \n",
    "donde \n",
    "- $P = [e_1 | \\dots | e_p]$ es la matriz de eigenvectores de $\\varSigma$, la matriz de covarianzas, tal que $\\varSigma = P \\varLambda P^{T}$.\n",
    "- $x$ vector renglón.\n",
    "- $\\mu$ el vector de medias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El problema de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " El problema de optimización en PCA consiste en resolver el siguiente sistema:\n",
    " \n",
    " $$\\max_{a_1^{T} a_1 = 1} a_1^{T} \\varSigma a_1 = Var(Y_1)$$\n",
    " \n",
    " \n",
    " $$\\max_{a_i^{T} a_i = 1} a_i^{T} \\varSigma a_i = Var(Y_i), i = 2, \\dots, n$$ <span style=\"color:green\">(Falta agregar la restricción $a_i^T \\varSigma a_k = 0, k<i$)</span>\n",
    " \n",
    "donde:\n",
    "- $x$ es un vector de tamaño $p \\times 1$ con matriz de covarianzas $\\varSigma$ y eigenvalores $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\lambda_p \\geq 0$.\n",
    "- $a_1, \\dots, a_p \\in \\mathbb{R}^p$ los coeficientes de las $p$ combinaciones lineales $Y_i = a_ix$\n",
    "- La norma $||a_i||=1$, para no incrementar de forma arbitraria $Var(Y_i)$, ya que esta varianza se podría incrementar arbitrariamente al multiplicar $a_i$ por alguna constante.\n",
    "- $a_i^T \\varSigma a_k = 0, k<i$ es equivalente a decir que $Cov(a_i^T X, a_k^T X) = 0, k<i$ (ya que buscamos que las componentes principales sean ortogonales).\n",
    "\n",
    "\n",
    " De modo que la primera componente principal es la combinación lineal con varianza máxima; es decir, que maximiza $Var(Y_1) = a_1^{T} \\varSigma a_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La solución a este problema de optimización es: $$a_i^* = e_i, Var(Y_i) = \\lambda_i$$\n",
    " \n",
    " Los vectores y valores propios de $\\varSigma$.\n",
    " \n",
    " Por lo que la $i$-ésima componente principal está dada por:\n",
    " \n",
    " $$\\hat{y_i} = \\hat{e_i}^Tx = \\hat{e_{i1}}x_1 + \\hat{e_{i2}}x_2 + \\dots + \\hat{e_{ip}}x_p$$\n",
    " \n",
    " donde $\\hat{e_i}$ es el eigenvector asociado al $i$-ésimo eigenvalor $\\hat{\\lambda_i}$, con $\\hat{\\lambda_1} \\geq \\hat{\\lambda_2} \\geq \\dots \\geq \\hat{\\lambda_p} \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notemos que $tr(\\varSigma) = \\sum_{i=1}^{p} \\lambda_i = \\sum_{i=1}^{p} Var(Y_i)$, por lo que la proporción de la varianza total explicada  de la i-ésima componente principal es:\n",
    " \n",
    " $$\\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p}, i = 1, \\dots, p$$\n",
    " \n",
    " Luego, si la mayor parte de la varianza total se puede atribuir a las primeras componentes, entonces con éstas componentes podemos *sustituir* a las $p$ variables, sin perder mucha información, a cambio de disminuir la dimensión de nuestro problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estandarización de variables\n",
    "2. Selección del número de componentes principales según el porcentaje de varianza explicada\n",
    "3. Utilizar el algoritmo de PCA, que podemos consultar en la documentación de [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "4. Interpretar las componentes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elección de las Componentes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No existe una regla definitiva para determinar el número de componentes principales con las que resolveremos cada problema particular, pero en general, podemos utilizar las siguientes guías:\n",
    "\n",
    "- Quedarnos con las CP que acumulen cierto porcentaje de la variación total.\n",
    "- Quedarnos con las CP cuyos eigenvalores sean mayores que el promedio $\\bar{\\lambda}$.\n",
    "- Utilizar una gráfica de codo (*scree plot*).\n",
    "- Verificar la significancia de las componentes más *grandes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación de las Componentes Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La interpretación de las **componentes principales** depende de cada problema particular, así como de la varianza explicada en cada una de las componentes. En este sentido, debemos de analizar para nuestro problema, para cada componente principal, con cuáles variables originales está fuertemente correlacionada, y si se incrementa o decrementa ante cambios en estas variables, puesto que la componente principal podemos interpretarla como un *resumen* de las variables originales con las que guarda una correlación (positiva o negativa) alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y Desventajas de PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas**:\n",
    "- Reducción de variables correlacionadas entre sí.\n",
    "- Mejora el desempeño del algoritmo, al trabajar con menos variables el tiempo de entrenamiento puede ser significativamente menor.\n",
    "- Reduce el sobreajuste.\n",
    "- Puede proveer una mejor visualización.\n",
    "\n",
    "\n",
    "**Desventajas**:\n",
    "- Las componentes principales pueden ser sensibles a outliers.\n",
    "- Las componentes principales son sensibles a distintos escalamientos de los datos originales (obtendemos distintas CP según las transformaciones que hagamos a los datos), puesto que el porcentaje de varianza explicado será distinto. Además, es necesario hacer una estandarización de las variables originales.\n",
    "- Difícil interpretación de las componentes principales y de las visualizaciones.\n",
    "- Puede haber una pérdida significativa de información si las componentes principales no son seleccionadas con cuidado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. S.L. Bruton, J.N. Kutz, [\"Data Driven Science & Engineering; Machine Learning, Dynamical Systems, and Control\"](http://databookuw.com/databook.pdf) p. 24-33, 2017.\n",
    "2. W. Wichern, R. Johnson, “Applied Multivariate Statistical Analysis” Fifth Edition. p. 430-465, April 1982.\n",
    "3. [Advantages and Disadvantages of Principal Component Analysis in Machine Learning](http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of_4.html) (March 2019) The Professionals Point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
